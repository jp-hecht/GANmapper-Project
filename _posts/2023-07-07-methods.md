---
layout: post
title: "Methods & Results"
subtitle: "What did we do to conduct our research question & How does it turn out"
date: 2023-02-31 10:45:13 -0400
background: '/img/posts/06.jpg'
author: Jonathan Hecht
---

# Objectives
Our approach is based on the idea of the GANmapper paper, which performs the generation of building footprints based on street types. As explained earlier, the original GANmapper paper has been applied to nine major cities around the world with varying degrees of success. In order to further test its capabilities and limitations, this paper aims to find out,
* a) to what extent the same model architecture can be applied to other population densities and urban morphologies.
* b) whether the model can be transferred to another city with new training data.

Hamburg and surrounding municipal region were chosen as an example of a new city because this area is familiar to us, which can help in estimating streets and situations and speed up the process. Regarding goal a), two datasets were prepared as described in [Data Chapter](2023-07-07-data.md). Since only the first dataset could be tested, only these experiments are discussed below. In the approach, the population density was taken as an indicator to classify into three classes based on it. These three classes are supposed to represent an approximation of densities and the resulting different morphologies. Three classes are only a simplification of densification degrees and different urban morphological types. In Germany alone, the regional statistical spatial topology partly knows 29 spatial types, where a distinction is made between main types and subtypes [^1]. This is only a small example that the classification by spatial types can be different depending on the content and context, and also a different class division can be useful.

> erläuterung zu drei klassen hier oder in data?

# Experiments
To answer the questions, the existing architecture of the GANmapper paper and its standard parameters were used. Only one experiment could be performed. The preparation and class formation of the data was already described in the chapter [Data](2023-07-07-data.md). The technical implementation was first done on a test basis on the local computer and the actual training later in [Google Colab](https://colab.research.google.com/). For this, the instructions for use in the [Github Repo](https://github.com/ualsg/GANmapper) for the GANmapper paper were followed. Some comments regarding the installation can be found in the [addtional resources](2023-07-07-additional.md). 

## Training process
After installing the repo, it becomes clear that there is a high number of different settings and (hyper-)parameters that can have an influence on the training process of the model. The different settings also influence each other. Since computational resources are limited, multiple settings could not be tested iteratively to find the best possible parameters[^6]. Since often the default settings in modeling give reasonable results since experience from previous modeling has already been incorporated, only the known settings from the experiments of the GANmapper paper are used here[^2]. This list can be found in Table 1. The most important parameters for training are now explained including the implied settings in the following. 

### Architecture
With regard to the architecture of the model, no adjustments were made to the approach used in the original paper[^2]. Accordingly, reference can also be made to their explanations. The architecture used corresponds to an image-to-image conditional GAN as propagated by Isola et al[^3]. The basic idea is that in many image processing, computer graphics, and computer vision problems an input image should be translated to an output image[^3]. This could be for example 'translating' a street network to the according building footprints.
The generator architecture itself follows an autoencoder structure. "An autoencoder[...] is trained to attempt to copy its input to its output."[^4]. Autoencoder are using an encoder-decoder structure to firstly reduce a high dimensional input to an internal representation of the input. Using this approach essential parts of the input could be extracted. The following decoder part is used to reconstruct the input from the internal representation as close as possible. In our case we are using 9 residual blocks between the encoder-decoder part of the generator[^5]. The basic ideas of these residual blocks to add skip connenctions, which enables the block to skip the activation of a layer and add up this activation on a later layer[^5]. Using this approach certain layers could be skipped, which are not useful for a improved result. 

The discriminator takes the generated output of the generator and the ground truth building footprint images as an input. Using a patchGAN the discriminator tries to classify the images to real and fake images[^3]. A patchGAN is basically a convolutional network, but the input is mapped to an NxN array (in our cas 70x70) instead of a single scalar. By means of the NxN array the whole input images are classified to a boolean value using the average of the array.

After passing the inputs through the generator and discriminator the loss for both is calculated and the weights are updated. After updating the process starts again and generator and discriminator should get better in their responsive roles. Both models get better until the generator output 'fools'/outperforms the verification of the discriminator for a real/fake image 50% of the time in this zero-sum game. In summary, the used architecture can be seen in the following figure:

<img class='img-fluid' src='/GANmapper-Project/img/posts/architecture_ganmapper.png' alt='Sample of output during the training process for three different models'>
<span class='caption text-muted'>Figure N: Used archtitecture during the experiments[^2]</span>

### Optimization
* Objective
* Loss



### Training process
* vlt. schon oben beschrieben
### Further things
* batch size
* intialization
* lr decay -> epoch
* why 300 epoch -> could get better or worse



# Results
* general notes

## Training sample
<img class='img-fluid' src='/GANmapper-Project/img/posts/example_train.drawio.png' alt='Sample of output during the training process for three different models'>
<span class='caption text-muted'>Figure N: Sample of output during the training process for three different models</span>


> model is training
>
> shapes are getting shaper
>
> just some fake images as an example and for getting a feeling
>
> known data

## Test sample

> just for the best model
> 
> other output was bad 
>
> 

<img class='img-fluid' src='/GANmapper-Project/img/posts/example_results.drawio.png' alt='Example training data for the second approach'>
<span class='caption text-muted'>Figure N: Sample test data results for high model</span>



## Loss curve

<img class='img-fluid' src='/GANmapper-Project/img/posts/loss_high_model.png' alt='Evolution of the loss function for the high density model'>
<span class='caption text-muted'>Figure N: Evolution of the loss function for the high density model</span>

## NOtes
* present results
* show samples
    * good one's
    * bad one's
* training loss

> Maybe seperated results folder



# Interpretation
* Analysis and results(without interpretation and discussion
* Interpretation
* Outcome of the experiment,take home message
> it is possible for hamburg & more 
> adapt
* Note: Analysis and discussion can also be combined ->
> what is meant with analysis



* Experiment objective(what should be demonstrated, why is the experiment conducted
> is the model able to be used for different degree of urbanism + morphology of a town
* Means to achieve the objective (can be included in the objective)
> wir adaptieren die ganze scheiße ohne parameter groß zu ändern 
* Task,e.g.,classification|detection|segmentation|.
> generation
* Optional: Sub-experiments or individual experiments (e.g.,parameter variation
> more or less default parameters
> mostly no results 


* which settings did we use from the ganmapper project
* limitations
* sample images
* used loss function
> how much explanation should i put here



* always ciombination of parameters
* after sorting out least these parameters seems to be relevant
* not including deeper changes of the code

* maybe as table -> flag name , explanition in their code -> our settings???




---


<div style="overflow-x:auto;">
<table>
<tbody>
<tr style="outline: thin solid">
<th>Flag name</th>
<th>Self description</th>
<th>Default value</th>
<th>Used value</th>
</tr>
<tr style="outline: thin solid">
<td>model</td>
<td>-</td>
<td>pix2pix</td>
<td>pix2pix</td>
</tr>
<tr style="outline: thin solid">
<td>ngf</td>
<td># of gen filters in the last conv layer</td>
<td>64</td>
<td>64</td>
</tr>
<tr style="outline: thin solid">
<td>ndf</td>
<td># of discrim filters in the first conv layer</td>
<td>64</td>
<td>64</td>
</tr>
<tr style="outline: thin solid">
<td>netD</td>
<td>specify discriminator architecture [basic | n_layers | pixel]. The basic model is a 70x70 PatchGAN. n_layers allows you to specify the layers in the discriminator</td>
<td>basic</td>
<td>basic</td>
</tr>
<tr style="outline: thin solid">
<td>netG</td>
<td>specify generator architecture [resnet_9blocks | resnet_6blocks | unet_256 | unet_128]</td>
<td>unet_256</td>
<td>resnet_9blocks</td>
</tr>
<tr style="outline: thin solid">
<td>norm</td>
<td>instance normalization or batch normalization [instance | batch | none]</td>
<td>instance</td>
<td>batch</td>
</tr>
<tr style="outline: thin solid">
<td>init_type</td>
<td>network initialization [normal | xavier | kaiming | orthogonal]</td>
<td>normal</td>
<td>normal</td>
</tr>
<tr style="outline: thin solid">
<td>init_gain</td>
<td>scaling factor for normal, xavier and orthogonal.</td>
<td>0.02</td>
<td>0.02</td>
</tr>
<tr style="outline: thin solid">
<td>no_dropout</td>
<td>no dropout for the generator</td>
<td>store_true</td>
<td>False</td>
</tr>
<tr style="outline: thin solid">
<td>serial_batches</td>
<td>if true, takes images in order to make batches, otherwise takes them randomly</td>
<td>store_true</td>
<td>False</td>
</tr>
<tr style="outline: thin solid">
<td>batch_size</td>
<td>input batch size</td>
<td>1</td>
<td>1</td>
</tr>
<tr style="outline: thin solid">
<td>load_size</td>
<td>scale images to this size</td>
<td>512</td>
<td>260</td>
</tr>
<tr style="outline: thin solid">
<td>crop_size</td>
<td>then crop to this size/td></td>
<td>512</td>
<td>256</td>
</tr>
<tr style="outline: thin solid">
<td>n_epochs</td>
<td>number of epochs with the initial learning rate</td>
<td>150</td>
<td>150</td>
</tr>
<tr style="outline: thin solid">
<td>n_epochs_decay</td>
<td>number of epochs to linearly decay learning rate to zero</td>
<td>150</td>
<td>150</td>
</tr>
<tr style="outline: thin solid">
<td>beta1</td>
<td>momentum term of adam</td>
<td>0.5</td>
<td>0.5</td>
</tr>
<tr style="outline: thin solid">
<td>lr</td>
<td>initial learning rate for adam</td>
<td>0.0002</td>
<td>0.0002</td>
</tr>
<tr style="outline: thin solid">
<td>gan_mode</td>
<td>the type of GAN objective. [vanilla| lsgan | wgangp]. vanilla GAN loss is the cross-entropy objective used in the original GAN paper.</td>
<td>lsgan</td>
<td>vanilla</td>
</tr>
<tr style="outline: thin solid">
<td>pool_size</td>
<td>the size of image buffer that stores previously generated images</td>
<td>50</td>
<td>0</td>
</tr>
<tr style="outline: thin solid">
<td>lr_policy</td>
<td>learning rate policy. [linear | step | plateau | cosine]</td>
<td>linear</td>
<td>linear</td>
</tr>
<tr style="outline: thin solid">
<td>lr_decay_iters</td>
<td>multiply by a gamma every lr_decay_iters iterations</td>
<td>50</td>
<td>-</td>
</tr>
</tbody>
</table>
</div>
<span class='caption text-muted'>Table 1: Main parameters for the training of the GANmapper Repo including their default value and our tested value</span>


---
#### References
[^1]: Federal Ministry of Transport and Digital Infrastructure (BMVI)(2021): Regional Statistical Spatial Typology for Mobility and Transport Research (RegioStaR). https://bmdv.bund.de/SharedDocs/DE/Anlage/G/regiostar-raumtypologie-englisch.pdf?__blob=publicationFile
[^2]: Wu, A. N. & F. Biljecki (2022): GANmapper: geographical data translation. International Journal of Geographical Information Science. 36(7), pages 1394-1422
[^3]: Isola, P., Zhu, J.-Y., Zhou, T. & A. A. Efros (2017): Image-to-Image Translation with Conditional Adversarial Networks. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, pages 5967-5976.
[^4]: Goodfellow, I., Begio, Y. & A. Courville (2016): Deep Learning. MIT Press.
[^5]: He, K.,Zhang, X.,Ren, S. & J. Sun (2016): Deep Residual Learning for Image Recognition, IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA, 2016, pages 770-778.
[^6]: Bergstra, J & Y. Bengio (2012): Random search for hyper-parameter optimization. J. Mach. Learn. Res. 13, pages 281–305.