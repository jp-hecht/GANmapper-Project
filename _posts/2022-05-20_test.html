---
layout: post 
title: 'some title'

---
<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">.lst-kix_h5i87bsp7wq9-2>li:before{content:"\0025a0   "}.lst-kix_h5i87bsp7wq9-4>li:before{content:"\0025cb   "}.lst-kix_h5i87bsp7wq9-1>li:before{content:"\0025cb   "}ul.lst-kix_h5i87bsp7wq9-6{list-style-type:none}ul.lst-kix_h5i87bsp7wq9-5{list-style-type:none}.lst-kix_7h6tq51jr22n-0>li:before{content:"\0025cf   "}ul.lst-kix_h5i87bsp7wq9-8{list-style-type:none}.lst-kix_h5i87bsp7wq9-5>li:before{content:"\0025a0   "}ul.lst-kix_h5i87bsp7wq9-7{list-style-type:none}ul.lst-kix_7h6tq51jr22n-0{list-style-type:none}ul.lst-kix_7h6tq51jr22n-1{list-style-type:none}ul.lst-kix_7h6tq51jr22n-2{list-style-type:none}.lst-kix_h5i87bsp7wq9-0>li:before{content:"\0025cf   "}ul.lst-kix_7h6tq51jr22n-3{list-style-type:none}.lst-kix_1nvmuy3lztj9-5>li:before{content:"\0025a0   "}.lst-kix_h5i87bsp7wq9-6>li:before{content:"\0025cf   "}.lst-kix_h5i87bsp7wq9-8>li:before{content:"\0025a0   "}ul.lst-kix_7h6tq51jr22n-4{list-style-type:none}ul.lst-kix_7h6tq51jr22n-5{list-style-type:none}ul.lst-kix_7h6tq51jr22n-6{list-style-type:none}ul.lst-kix_7h6tq51jr22n-7{list-style-type:none}.lst-kix_1nvmuy3lztj9-4>li:before{content:"\0025cb   "}.lst-kix_h5i87bsp7wq9-7>li:before{content:"\0025cb   "}ul.lst-kix_7h6tq51jr22n-8{list-style-type:none}ul.lst-kix_1nvmuy3lztj9-3{list-style-type:none}ul.lst-kix_1nvmuy3lztj9-4{list-style-type:none}ul.lst-kix_1nvmuy3lztj9-5{list-style-type:none}ul.lst-kix_1nvmuy3lztj9-6{list-style-type:none}ul.lst-kix_1nvmuy3lztj9-0{list-style-type:none}.lst-kix_7h6tq51jr22n-8>li:before{content:"\0025a0   "}ul.lst-kix_1nvmuy3lztj9-1{list-style-type:none}ul.lst-kix_1nvmuy3lztj9-2{list-style-type:none}.lst-kix_1nvmuy3lztj9-6>li:before{content:"\0025cf   "}ul.lst-kix_h5i87bsp7wq9-2{list-style-type:none}ul.lst-kix_h5i87bsp7wq9-1{list-style-type:none}.lst-kix_1nvmuy3lztj9-7>li:before{content:"\0025cb   "}ul.lst-kix_h5i87bsp7wq9-4{list-style-type:none}ul.lst-kix_h5i87bsp7wq9-3{list-style-type:none}.lst-kix_1nvmuy3lztj9-8>li:before{content:"\0025a0   "}ul.lst-kix_h5i87bsp7wq9-0{list-style-type:none}.lst-kix_h5i87bsp7wq9-3>li:before{content:"\0025cf   "}ul.lst-kix_vvaweaw4rsud-4{list-style-type:none}ul.lst-kix_vvaweaw4rsud-5{list-style-type:none}ul.lst-kix_vvaweaw4rsud-2{list-style-type:none}ul.lst-kix_vvaweaw4rsud-3{list-style-type:none}ul.lst-kix_vvaweaw4rsud-8{list-style-type:none}ul.lst-kix_vvaweaw4rsud-6{list-style-type:none}ul.lst-kix_vvaweaw4rsud-7{list-style-type:none}.lst-kix_vvaweaw4rsud-4>li:before{content:"-  "}.lst-kix_vvaweaw4rsud-5>li:before{content:"-  "}.lst-kix_vvaweaw4rsud-0>li:before{content:"-  "}.lst-kix_vvaweaw4rsud-8>li:before{content:"-  "}ul.lst-kix_1nvmuy3lztj9-7{list-style-type:none}ul.lst-kix_1nvmuy3lztj9-8{list-style-type:none}.lst-kix_vvaweaw4rsud-6>li:before{content:"-  "}.lst-kix_vvaweaw4rsud-7>li:before{content:"-  "}.lst-kix_7h6tq51jr22n-7>li:before{content:"\0025cb   "}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_7h6tq51jr22n-5>li:before{content:"\0025a0   "}.lst-kix_7h6tq51jr22n-6>li:before{content:"\0025cf   "}.lst-kix_1nvmuy3lztj9-1>li:before{content:"\0025cb   "}.lst-kix_1nvmuy3lztj9-3>li:before{content:"\0025cf   "}ul.lst-kix_vvaweaw4rsud-0{list-style-type:none}ul.lst-kix_vvaweaw4rsud-1{list-style-type:none}.lst-kix_7h6tq51jr22n-4>li:before{content:"\0025cb   "}.lst-kix_1nvmuy3lztj9-2>li:before{content:"\0025a0   "}.lst-kix_vvaweaw4rsud-1>li:before{content:"-  "}.lst-kix_7h6tq51jr22n-1>li:before{content:"\0025cb   "}.lst-kix_7h6tq51jr22n-2>li:before{content:"\0025a0   "}.lst-kix_vvaweaw4rsud-2>li:before{content:"-  "}.lst-kix_vvaweaw4rsud-3>li:before{content:"-  "}.lst-kix_7h6tq51jr22n-3>li:before{content:"\0025cf   "}.lst-kix_1nvmuy3lztj9-0>li:before{content:"\0025cf   "}ol{margin:0;padding:0}table td,table th{padding:0}.c0{margin-left:36pt;padding-top:12pt;padding-left:0pt;padding-bottom:12pt;line-height:1.15;orphans:2;widows:2;text-align:justify}.c4{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:justify;height:12pt}.c3{padding-top:0pt;text-indent:36pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:justify}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Times New Roman";font-style:normal}.c2{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c9{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Times New Roman";font-style:normal}.c6{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:justify}.c5{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:justify}.c8{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:justify}.c13{background-color:#ffffff;max-width:451.4pt;padding:72pt 72pt 72pt 72pt}.c7{padding:0;margin:0}.c11{width:33%;height:1px}.c12{text-indent:36pt}.c10{font-style:italic}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Times New Roman";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:justify}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:justify}li{color:#000000;font-size:12pt;font-family:"Times New Roman"}p{margin:0;color:#000000;font-size:12pt;font-family:"Times New Roman"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Times New Roman";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:justify}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Times New Roman";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:justify}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Times New Roman";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:justify}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Times New Roman";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:justify}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Times New Roman";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:justify}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Times New Roman";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:justify}</style></head><body class="c13 doc-content"><h2 class="c6" id="h.501ssiw8fcy5"><span class="c9">What is a Generative Adversarial Network ? </span></h2><p class="c5"><span class="c2">Phrase d&rsquo;accroche : </span></p><p class="c3"><span class="c2">One of the most interesting deep learning projects in computer science today is the development and use in multiple forms of Generative Adversarial Networks. In this article, we give you an overview of how it works and what it can be used for. (&agrave; retravailler)</span></p><p class="c4"><span class="c2"></span></p><p class="c3"><span>The idea of a Generative Adversarial Networks (GAN) was born thanks to a group of researchers trying to find a solution on how to make a model learn better and faster. Among them was Ian Goodfellow who wrote the first article to present and explain this new model in 2014</span><sup><a href="#ftnt1" id="ftnt_ref1">[1]</a></sup><span class="c2">.</span></p><p class="c5"><span class="c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;As it is written in its name a GAN is a generative model based on deep learning model architecture. Unlike a discriminative model, which associates data according to the input and output, the generative model has the capacity to generate new data close to the input data. For example, if we give him a picture of a cat and explain that it&#39;s a cat it is supposed to be able to generate a picture of a cat close to reality so that a discriminative model wouldn&rsquo;t be able to tell if it&rsquo;s fake. This is why this kind of model is also defined as an unsupervised model since it&rsquo;s supposed to discover and learn by itself the regularities and patterns of the dataset input and generate a new output close to it. </span></p><p class="c4"><span class="c2"></span></p><p class="c5"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In this way, the GAN is considered as a solution to train generative models. It is composed of two neural networks : a generator and a discriminator. The generator is the network in charge of generating new data. It could be seen as the artist learning how to reproduce the art of well-known artists. The discriminator is the network in charge of distinguishing between real and fake data. It is as an art critic trying to differentiate art pieces between those of true artists and those of </span><span>contrafactors</span><span class="c2">. These two neural networks are competing against each other until the discriminator can&rsquo;t recognize the real ones from the fake ones. Their learning pattern is close to a zero sum game where the improvement of one comes at the loss of the other. Here, the more the generator becomes efficient the more the discriminator&rsquo;s capacity is reduced and frozen with a 50% percent chance of differentiating the fake from the real ones. </span></p><p class="c4"><span class="c2"></span></p><p class="c5"><span class="c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The model architecture of GAN is based on a two steps learning process that isn&rsquo;t running at the same time to prevent any form of confusion for each neural network. On one side there is the discriminator. To make it run it needs an input of real data and an input of fake data generated by the generator. During the training phase those images will appear randomly and the discriminator will learn how to differentiate them. At the beginning the exercise is really easy for it because the generator isn&rsquo;t able to create data close to the real one (PICTURE LEARNING STEP). </span></p><p class="c3"><span class="c2">Nevertheless, as the generator is training, the task becomes more and more difficult for the discriminator. Everytime it doesn&rsquo;t succeed in differentiate from real to fake ones, it has to upload his weight thanks to backpropagation (PICTURES)</span></p><p class="c4 c12"><span class="c2"></span></p><p class="c3"><span class="c2">On the other side, there is the generator&rsquo;s network. First we give him some random input, usually noise because it&rsquo;s the easiest material to generate anything out of it. However, it is not the case for every GAN, some of them are using a specific input to generate a precise output. After creating the image, it sends it to the discriminator which will do his part. If the generator can&rsquo;t fool the discriminator then, the backpropagation will pass through the discriminator to send the information to the generator and this is where the two neural networks are connected. This is why the weights of the discriminator are changed only when the generator isn&rsquo;t working, otherwise it wouldn&#39;t be able to send any data to the discriminator. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The progress of the model can be checked with the loss functions. Usually, a GAN has two loss functions, one for each neural network and they measure the distance between probability distributions. </span></p><p class="c4 c12"><span class="c2"></span></p><p class="c3"><span class="c2">&nbsp;Today, there are a lot of ways to use GAN in multiple fields because it would be able to generate sounds, videos, images and more. Actually, they are mainly used to generate images or pictures as the model &ldquo;this person doesn&rsquo;t not exist&rdquo; which is able to generate faces of people who don&#39;t exist. The same technique is used for deep fakes except that this time the generator is creating pictures of someone who does exist and try to be really close to reality so that we can&rsquo;t tell if it&rsquo;s real or not. These uses also raise ethical questions about the possibility of using someone&#39;s image as we see fit.</span></p><p class="c3"><span>Another impressive way to use GAN is in the medical field where the researchers have made a lot of progress. In the article </span><span class="c10">Medical Image Generation Using Generative Adversarial Networks: A Review</span><sup><a href="#ftnt2" id="ftnt_ref2">[2]</a></sup><span>, the researchers explain the progress made in the medical field using image generation and the principle uses. One of them is the generation of training samples produced from original images. With GAN, the medical field can extend their image bank without having to take pictures of real patients which need their consent. Another way of using GAN is in image to image translation. In fact, there are multiple techniques for image acquisition with a lot of different imaging modalities such as &ldquo;ultrasonography, computed tomography (CT), positron emission tomography (PET), and magnetic resonance imaging (MRI)&rdquo;</span><sup><a href="#ftnt3" id="ftnt_ref3">[3]</a></sup><span class="c2">&nbsp;and as the researchers explained the complexity and data processing is different in each case and it&#39;s even more complex when they want to create hybrid images using two different techniques. Afterward the extraction of one modality is also complicated since an automated analysis needs high-quality images and clear features. Therefore, they are trying to develop a GAN model that would be able to translate one image modality from another.</span></p><p class="c3"><span>For all those examples, a different kind of GAN can be used. In our study, we will focus on the conditional GAN. As explained in the paper </span><span class="c10">Conditional generative adversarial nets</span><sup><a href="#ftnt4" id="ftnt_ref4">[4]</a></sup><span>, &ldquo;In an unconditioned generative model, there is no control on modes of the data being generated. However, by conditioning the model on additional information it is possible to direct the data generation process&rdquo;</span><sup><a href="#ftnt5" id="ftnt_ref5">[5]</a></sup><span class="c2">. These additional informations can be from any kind as class labels or data from other modalities and are here to give a direction to the generator&rsquo;s task. For example, we can ask a GAN to generate pictures of cakes but if we are only interested in cheesecakes, we can add other information that will help him to generate only pictures of different kinds of cheesecakes. As a result, the generator is more specific and the discriminator is better which is making the model efficient. The conditional Gan&rsquo;s architecture is the same as the original except that there is the additional informations named as label in this diagram : (DIAGRAM)</span></p><p class="c4 c12"><span class="c2"></span></p><p class="c3"><span class="c2">With conditional GAN it is possible to create a model such as image-super-resolution which is able to conceive the same image as in the data input but with a better quality or the text-to-image resolution, a model able to design a picture based on what the speaker describes in text as nightcaf&eacute;, mid journey, etc.</span></p><p class="c3"><span class="c2">The one we will focus on in this study is the image-to-image translation which is usually the transformation of one picture to another with more features or different designs as they are used in the medical field to generate sample images. See you at the next chapter !</span></p><p class="c4 c12"><span class="c2"></span></p><p class="c4 c12"><span class="c2"></span></p><p class="c4 c12"><span class="c2"></span></p><p class="c4"><span class="c2"></span></p><hr style="page-break-before:always;display:none;"><p class="c4"><span class="c2"></span></p><h2 class="c6" id="h.pjjq7ucuc1dt"><span class="c9">References : </span></h2><ul class="c7 lst-kix_1nvmuy3lztj9-0 start"><li class="c0 li-bullet-0"><span class="c2">CNRS - Formation FIDLE, Seq.13 &quot;Generative Adversarial Networks (GAN)&quot;, march 2023, https://www.youtube.com/watch?v=hvFthCbTl5c&amp;t=2854s&amp;ab_channel=CNRS-FormationFIDLE</span></li><li class="c0 li-bullet-0"><span class="c2">Goodfellow, I., Pouget-Abadie, J, Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., ... &amp; Bengio, Y. (2020): Generative adversarial networks. Communications of the ACM, 63(11), 139-144.</span></li><li class="c0 li-bullet-0"><span class="c2">Isola, P., Zhu, J. Y., Zhou, T., &amp; Efros, A. A. (2017):Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1125-1134).</span></li><li class="c0 li-bullet-0"><span class="c2">Mirza, M., &amp; Osindero, S. (2014): Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784</span></li><li class="c0 li-bullet-0"><span class="c2">Singh, Nripendra &amp; Raza, Khalid. (2021): Medical Image Generation Using Generative Adversarial Networks: A Review.</span></li></ul><p class="c4"><span class="c2"></span></p><hr style="page-break-before:always;display:none;"><p class="c4"><span class="c2"></span></p><h2 class="c6" id="h.wh05fmtnj1"><span class="c9">What is GANMapper ? </span></h2><p class="c3"><span class="c2">The GANMapper is a conditional GAN (refer to last article),issued by two researchers : Abraham Noah Wu a phD researcher at National University of Singapore who has a master degree in Architecture, studying new uses of urban data using deep learning and Filip Biljecki, an assistant professor at National University of Singapore who realized is phD degree in 3D GIS and G&eacute;omatic engineer. </span></p><p class="c4 c12"><span class="c2"></span></p><p class="c3"><span class="c2">rapid review geographic GAN </span></p><p class="c4"><span class="c2"></span></p><hr class="c11"><div><p class="c8"><a href="#ftnt_ref1" id="ftnt1">[1]</a><span class="c1">&nbsp;Goodfellow, I., Pouget-Abadie, J, Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., ... &amp; Bengio, Y. (2020): Generative adversarial networks. Communications of the ACM, 63(11), 139-144.</span></p></div><div><p class="c8"><a href="#ftnt_ref2" id="ftnt2">[2]</a><span class="c1">&nbsp;Singh, Nripendra &amp; Raza, Khalid. (2021): Medical Image Generation Using Generative Adversarial Networks: A Review.</span></p></div><div><p class="c8"><a href="#ftnt_ref3" id="ftnt3">[3]</a><span class="c1">&nbsp;ibid page 78</span></p></div><div><p class="c8"><a href="#ftnt_ref4" id="ftnt4">[4]</a><span class="c1">&nbsp;Mirza, M., &amp; Osindero, S. (2014): Conditional generative adversarial nets</span></p></div><div><p class="c8"><a href="#ftnt_ref5" id="ftnt5">[5]</a><span class="c1">&nbsp;ibid page 1</span></p></div></body></html>