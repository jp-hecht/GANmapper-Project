---
layout: post
title: 'Data'
subtitle: 'What data did we used & how we pre-processed it?'
date: 2023-02-31 10:45:13 -0400
background: '/img/data_bw.jpg'
author: Jonathan Hecht
---
To answer the research question, two different approaches for data preparation and training of the model were performed. Both approaches aim to include the known requirements and known information from the original GANmapper article [^1]. The second attempt could not be tested afterwards, but it is a further development of the first approach and accordingly takes over large parts of the first approach. The background why the second approach could not be tested in practice is shown in [Discussion](2023-07-07-discussion.md).

## Known requirements from the GANmapper paper
As is already known, within the Ganmapper paper different experiments were tested with respect to the data and their configuration. Here we only follow their best results in order to avoid duplicate experiments. From the paper, the appropriate settings can be summarized as follows:

* OpenStreetMap (OSM) building footprints and street data are used as a data basis
* Converting basic street data to Coloured Road Hierarchy Diagrams (CRHD) proposed by Chen et al. (2021)[^2]
* Using not all street types from OSM 
* Zoom level 16 of the [Slippy map format](https://wiki.openstreetmap.org/wiki/Slippy_map)/XYZ-Tiles as a compromise between visual sharpness and contextual information
* Source and target data pairs
* 256x256 resolution per source and target tile

## First approach
First, the OSM data of Hamburg, Lower Saxony and Schleswig-Holstein were downloaded from the website [www.geofabrik.de](https://download.geofabrik.de/) and merged to one layer. Subsequently, the streets and building footprints were trimmed to the minimum bounding box of the administrative boundaries of Hamburg. In order to differentiate by population densities, corresponding data was downloaded from [eurostat](https://ec.europa.eu/eurostat/web/gisco/geodata/reference-data/population-distribution-demography/geostat)[^3] and cropped. The subdivision by population densities was done by dividing the eurostat data set into three quantiles (<33%,>33%-<66%,>66%). This is to simplify a subdivision into low, medium and high population densities. The corresponding distribution including the threshold values can be seen here:

<img class='img-fluid' src='/GANmapper-Project/img/posts/hist_pop_2018.svg' alt='Histogram of the population density of Hamburg and surroundings including three quantiles in 2018'>
<span class='caption text-muted'>Figure 1: Histogram of the population density of Hamburg and surroundings including three quantiles in 2018</span>
The subdivision leads into this following map:
<img class='img-fluid' src='/GANmapper-Project/img/posts/map_pop_2018.svg' alt='Area of Interest and the according population density'>
<span class='caption text-muted'>Figure 2: Area of Interest and the according population density</span>
Subsequently, the street and building data were cut by the respective population density. Before the XYZ tiles could be generated as source and target data pairs, the streets and buildings had to be visualized according to the specifications. With respect to the buildings, the footprints were simply colored black. For the different street types that are represented as CRHD, the [GitHub Repository](https://github.com/ualsg/Road-Network-Classification/blob/main/crhd_generator.py) by Chen et al. (2021)[^2] was used. For this, the assumption had to be made that the authors of the GANmapper paper used exactly the corresponding street lines and colors. In terms of line width, the ratios of the repository were picked up and the starting width was estimated. Subsequently, the source and target data for each population density could be generated around XYZ format. Some examples can be seen here:
<img class='img-fluid' src='/GANmapper-Project/img/example_approach_1.drawio.png' alt='Example training data for the first approach'>
<span class='caption text-muted'>Figure 3: Example training data for the first approach</span>
These data pairs were pre-processed in the last step using the existing script. The main steps of the process were:
* Sorting out images without building footprints.
* Sorting out images with few building footprints or in general images with more white pixels included. The described values of the repo of a maximum of 170000 white pixels at zoom level 16 were used, i.e. with
    * <math>
        <mrow> 
            <mrow>  
                <mn>256</mn>  
                <mo>&sdot;</mo>  
                <mn>256</mn>
                <mo>&sdot;</mo> 
                <mn>3</mn>
            </mrow>
            <mo>=</mo>  
            <mn>196608</mn> 
            <mspace width="5px" />
            <mi> total pixels per image</mi>
        </mrow>
        </math>
    * <math>
        <mfrac>
            <mrow>
            <mn>170000</mn>
            </mrow>
            <mn>196608</mn>
        </mfrac>
        <mo>&sdot;</mo>
        <mn>100</mn>
        <mo>&asymp;</mo>
        <mn>86.45%</mn>
        <mspace width="5px" />
        <mi>maximum amount of white pixels, leads to at least </mi>
        <mspace width="5px" />
        <mn>13.55%</mn>
        <mspace width="5px" />
        <mi>coloured pixels. </mi>
        </math>
* In the next step, the total data was divided into training, testing and validation data (70%,15%,15%). 
* At last, the source and target data were merged to form a larger image. 
After pre-processing starting from a total amount of about 12600 source images, the number of image pairs per population density can be obtained from Table 1.
<div style="overflow-x:auto;">
  <table>
    <tr>
        <th>Population Density</th>
        <th>Tiles after removing blank tiles</th>
        <th>Training tiles after pre-processing</th>
    </tr>
    <tr>
        <td>High</td>
        <td>3765</td>
        <td>1758</td>
    </tr>
    <tr>
        <td>Medium</td>
        <td>3110</td>
        <td>311</td>
    </tr>
        <tr>
        <td>Low</td>
        <td>1690</td>
        <td>72</td>
    </tr>
  </table>
</div>
<span class='caption text-muted'>Table 1: Number of data pairs after pre-processing per population density</span>

### Drawbacks of this approach 
Although this approach was used and the results were partly generated by means of these data. However, it was already determined in the course of the processing that there are some disadvantages, which are to be explained here already, in order to show afterwards the alternative and improved approach of the generation of data.
First of all, the up-to-dateness and especially the spatial resolution of the population data must be criticized. The spatial resolution leads to border effects where areas of the individual population polygons of the raster tiles are cut off (cf. Figure 3). In addition, within the population distribution partly no data are available (cf. Figure 2), whereby this does not include of course that there can be no buildings. Thus, such a cropping of the data already leads to the loss of possible training data. In addition, it can be seen from Table 1 that the sorting out of images with insufficiently colored pixels leads to a very small number of training pairs and that the existing training pairs within the population density classes are similar on the outside. Thereby, the numbers are probably not sufficient for an ordinary training process. The impact on comparability between datasets and the general impact on the question will be addressed in more detail in [discussion](2023-07-07-discussion.md).
The last thing to criticize is the use of only the proposed street types. Only 11 of 28 possible street types in Hamburg were used.

## Differences for the second approach
Regarding the amount of disadvantges und the missing comparability due to the different amount of training data, it seems to be reasonable to change the pre-processing. The basic process remains the same, but the division was not based on the population density of a dataset, but solely on the pixel distribution. Secondly, the street types were examined more closely and additional street types were added.
Before the distribution of the white pixels could be examined in more detail, it was first necessary to determine which street types of the OSM data should be used. The summation of the lengths results in the following figure:
<img class='img-fluid' src='/GANmapper-Project/img/posts/osm_hh_street_hist.svg' alt='Histogram of the length of different street types in Hamburg and Surroundings'>
<span class='caption text-muted'>Figure 4: Histogram of the length of different street types in Hamburg and Surroundings</span>
In the previous approach were already used, the following street types:
```python
street_types = ['service', 'residential', 'tertiary_link', 'tertiary', 'secondary_link', 'primary_link',
                 'motorway_link', 'secondary', 'trunk', 'primary', 'motorway']
```
Based on the distribution it was decided to add 'unclassified' and 'living_street'. Many of the street segments with shorter path lengths, such as 'bridleway' were not used due to these. For other street classes, such as 'cycleway' or 'footway' the visual check seemed to show a lot of duplication with the other street types. In addition, these did not appear to be complete in large parts. Subsequently, XYZ tiles were generated again. The line thickness was slightly increased.

Subsequently, the data set had to be divided. The following figure shows the accumulation of white pixels within the entire initial data set including the division into three quantiles.
<img class='img-fluid' src='/GANmapper-Project/img/posts/white_pixel_hist.svg' alt='Histogram of the total amount of white pixels in Hamburg and Surroundings'>
<span class='caption text-muted'>Figure 5: Histogram of the total amount of white pixels in Hamburg and Surroundings</span>
Thereby the number of pixels without color value becomes clear by for example waters or forests. Subsequently, these values were used to perform a slightly modified pre-processing with the corresponding values. The adapted function looks as follows:

```python
def combine_target_mask(mask_file_path, target_file_path, threshold_1,threshold_2, output_folder_high, output_folder_medium, output_folder_low,i):   
    img1 = cv2.imread(mask_file_path)
    img2 = cv2.imread(target_file_path)
    # resize
    img1 = cv2.resize(img1, (256,256))    
    img2 = cv2.resize(img2, (256,256))    
    # high
    if np.unique(img2, return_counts=True)[1][-1] > threshold_1:   
        h1, w1 = img1.shape[:2]
        h2, w2 = img2.shape[:2]
        #create empty matrix
        vis = np.zeros((max(h1, h2), w1+w2,3), np.uint8)
        #combine 2 images
        vis[:h1, :w1,:3] = img1
        vis[:h2, w1:w1+w2,:3] = img2
        cv2.imwrite(output_folder_high + str(i) + '.png', vis)
    # medium
    elif np.unique(img2, return_counts=True)[1][-1] <= threshold_1 and np.unique(img2, return_counts=True)[1][-1] >= threshold_2:
        h1, w1 = img1.shape[:2]
        h2, w2 = img2.shape[:2]
        vis = np.zeros((max(h1, h2), w1+w2,3), np.uint8)
        vis[:h1, :w1,:3] = img1
        vis[:h2, w1:w1+w2,:3] = img2    
        cv2.imwrite(output_folder_medium + str(i) + '.png', vis)
    # low
    elif np.unique(img2, return_counts=True)[1][-1] < threshold_2:
         h1, w1 = img1.shape[:2]
         h2, w2 = img2.shape[:2]
         vis = np.zeros((max(h1, h2), w1+w2,3), np.uint8)
         vis[:h1, :w1,:3] = img1
         vis[:h2, w1:w1+w2,:3] = img2
         cv2.imwrite(output_folder_low + str(i) + '.png', vis)
    else:
         vis = None
    return vis

```
<span class='caption text-muted'>Listing 1: Changed combine_target_mask function for the second approach</span>
With this type of pre-processing, the population density is only indirectly inferred and the pixel density, especially since streets are also included, is only an indicator for a high population density. However, the following figure shows that at least in parts a lower population density should apply. In addition, compared to the first approach, the artifacts could be almost completely eliminated by splitting. A comparable effect can also be found in the number of training data.

<img class='img-fluid' src='/GANmapper-Project/img/posts/example_approach_2.drawio.png' alt='Example training data for the second approach'>
<span class='caption text-muted'>Figure 6: Example training data for the second approach</span>

<div style="overflow-x:auto;">
  <table>
    <tr>
        <th>Amount of Colored Pixels&nbsp;&nbsp;&nbsp;</th>
        <th>Train Dataset&nbsp;&nbsp;&nbsp;</th>
        <th>Test Dataset&nbsp;&nbsp;&nbsp;</th>
        <th>Validation Dataset&nbsp;&nbsp;&nbsp;</th>
    </tr>
    <tr>
        <td>High</td>
        <td>2503</td>
        <td>313</td>
        <td>313</td>
    </tr>
    <tr>
        <td>Medium</td>
        <td>2503</td>
        <td>313</td>
        <td>313</td>
    </tr>
        <tr>
        <td>Low</td>
        <td>2578</td>
        <td>322</td>
        <td>323</td>
    </tr>
  </table>
</div>
<span class='caption text-muted'>Table 2: Number of data pairs after pre-processing by pixel value</span>

---
# Notes & personal notes
>
> (citation -> maybe direct link & reference website with fixed entries??) -> yes but with footnote
>
> citation with pages or without?
>
> genrellen theoretischen überbau erklären -> also warum quell und ziel daten etc. -> von grob zu klein
>
> be careful with the length of the text 
>
> immer wenn ich kritisiere sich auf andere stellen bezoehen oder direkt mit quellen erklären warum das problematisch ist
> 
> do we have the not preprint version of the ganmapper paper -> since i would like to put the page information in the references....

---
#### References
[^1]: Wu, A. N. & F. Biljecki (2022): GANmapper: geographical data translation. International Journal of Geographical Information Science. 36(7), pages 1394-1422
[^2]: Chen, W., Wu, A., N. & F. Biljecki (2021): Classification of urban morphology with deep learning: Application on urban vitality. Computers, Environment and Urban Systems. 90(101706).
[^3]: Batista e Silva F, Dijkstra L, Poelman H (2021): The JRC-GEOSTAT 2018 population grid. JRC Technical Report. Forthcoming.